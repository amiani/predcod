{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of zero-divergence Inference Learning in a Predictive Coding Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Coding Network\n",
    "\n",
    "A predictive coding network is a probabilistic model that calculates.\n",
    "\n",
    "Variables on adjacent levels are assumed to be related by\n",
    "\n",
    "$$ P(x_i^l | \\bar x^{l+1}) = \\mathcal{N}( x_i^l; \\mu_i^l, \\Sigma_i^l) $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\mu_i^l = {\\theta_i^l}^T f(\\bar x^{l+1}) $$\n",
    "\n",
    "with the objective being to maximize\n",
    "\n",
    "$$ F = \\ln P(\\bar x^1,...,\\bar x^{L-1} | \\bar x^L) $$\n",
    "\n",
    "due to the assumed relationship between adjacent layers this simplifies to\n",
    "\n",
    "$$ \\begin{align*}\n",
    "    F &= \\sum_{l=0}^{L-1} \\ln P(\\bar x^l | \\bar x^{l+1})    \\\\\n",
    "    &= \\sum_{l=0}^{L-1} \\sum_{i=1}^n \\ln \\mathcal{N}( x_i^l; \\mu_i^l, \\Sigma_i^l)    \\\\\n",
    "    &= \\sum_{l=0}^{L-1} \\sum_{i=1}^n \\ln \\frac{1}{\\sqrt{2\\pi\\Sigma_i^l}} - \\frac{1}{2}\\frac{(x_i^l - \\mu_i^l)^2}{\\Sigma_i^l}\n",
    "\\end{align*} $$\n",
    "\n",
    "ignoring the constant term (since we are going to use the derivative with respect to $x_i^l$)\n",
    "\n",
    "$$ F = -\\frac{1}{2} \\sum_{l=0}^{L-1} \\sum_{i=1}^n \\frac{(x_i^l - \\mu_i^l)^2}{\\Sigma_i^l} $$\n",
    "\n",
    "In this model we will assume the variances to be 1, and letting $\\epsilon_i^l = x_i^l - \\mu_i^l$\n",
    "\n",
    "$$ F = -\\frac{1}{2} \\sum_{l=0}^{L-1} \\sum_{i=1}^n (\\epsilon_i^l)^2 $$\n",
    "\n",
    "to update each $x_i$ we will use the partial derivative of $F$ with respect to $x_i$\n",
    "\n",
    "$$ \\frac{\\partial F}{x_i^l} = -\\epsilon_i^l + f'(x_i^l) \\sum_{k=1}^n \\theta_{i,k}^l \\epsilon_k^{l-1}$$\n",
    "\n",
    "updating the weights\n",
    "$$ \\frac{\\partial F}{\\theta_{i,j}^l} = \\epsilon_i^{l-1} f(x_j^l) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PredCodMLP import PredCodMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train:  60000\n",
      "num_test:  10000\n"
     ]
    }
   ],
   "source": [
    "from read_image import *\n",
    "\n",
    "train_images = read_mnist_images('data/train-images-idx3-ubyte.gz')\n",
    "train_labels = read_mnist_labels('data/train-labels-idx1-ubyte.gz')\n",
    "test_images = read_mnist_images('data/t10k-images-idx3-ubyte.gz')\n",
    "test_labels = read_mnist_labels('data/t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "num_train, H, W = train_images.shape\n",
    "num_test, _, _ = test_images.shape\n",
    "train_images = train_images.reshape(num_train, 1, H, W)\n",
    "test_images = test_images.reshape(num_test, 1, H, W)\n",
    "print(\"num_train: \", num_train)\n",
    "print(\"num_test: \", num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100 # number of points per class\n",
    "D = 2 # dimensionality\n",
    "K = 3 # number of classes\n",
    "X = np.zeros((N*K,D)) # data matrix (each row = single example)\n",
    "y = np.zeros(N*K, dtype='uint8') # class labels\n",
    "for j in range(K):\n",
    "  ix = range(N*j,N*(j+1))\n",
    "  r = np.linspace(0.0,1,N) # radius\n",
    "  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n",
    "  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "  y[ix] = j\n",
    "# lets visualize the data:\n",
    "#plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1077205\n",
      "1.104997\n",
      "1.1022948\n",
      "1.0996084\n",
      "1.0969403\n",
      "1.0942934\n",
      "1.0916653\n",
      "1.0890595\n",
      "1.0864744\n",
      "1.08391\n",
      "1.0813674\n",
      "1.078839\n",
      "1.0763226\n",
      "1.0738118\n",
      "1.071308\n",
      "1.0688065\n",
      "1.066311\n",
      "1.0638121\n",
      "1.061314\n",
      "1.0588183\n",
      "1.0563209\n",
      "1.0538211\n",
      "1.0513207\n",
      "1.0488204\n",
      "1.0463135\n",
      "1.0437995\n",
      "1.0412705\n",
      "1.038726\n",
      "1.0361686\n",
      "1.0335956\n",
      "1.031004\n",
      "1.0283917\n",
      "1.0257543\n",
      "1.0230974\n",
      "1.0204182\n",
      "1.0177125\n",
      "1.014976\n",
      "1.0122107\n",
      "1.0094197\n",
      "1.0066029\n",
      "1.0037632\n",
      "1.0009016\n",
      "0.99801546\n",
      "0.99510807\n",
      "0.992179\n",
      "0.9892277\n",
      "0.98625916\n",
      "0.983273\n",
      "0.98026747\n",
      "0.9772445\n",
      "0.97420543\n",
      "0.97115195\n",
      "0.9680845\n",
      "0.96500325\n",
      "0.9619074\n",
      "0.9587969\n",
      "0.9556717\n",
      "0.9525391\n",
      "0.9493984\n",
      "0.94624674\n",
      "0.94308424\n",
      "0.93991333\n",
      "0.9367353\n",
      "0.9335524\n",
      "0.93036443\n",
      "0.9271734\n",
      "0.92398113\n",
      "0.9207888\n",
      "0.9175976\n",
      "0.91440797\n",
      "0.91122204\n",
      "0.90803766\n",
      "0.90485567\n",
      "0.90168285\n",
      "0.8985194\n",
      "0.8953656\n",
      "0.8922158\n",
      "0.88907087\n",
      "0.88593835\n",
      "0.8828171\n",
      "0.8797103\n",
      "0.8766214\n",
      "0.8735497\n",
      "0.8705008\n",
      "0.86747175\n",
      "0.8644668\n",
      "0.8614857\n",
      "0.8585311\n",
      "0.85560405\n",
      "0.8527055\n",
      "0.84983444\n",
      "0.8469903\n",
      "0.84417474\n",
      "0.84138936\n",
      "0.8386341\n",
      "0.83591145\n",
      "0.8332202\n",
      "0.83056116\n",
      "0.8279337\n",
      "0.8253402\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, random, value_and_grad\n",
    "from typing import Tuple\n",
    "from adam import adam, make_default_adam_config\n",
    "\n",
    "def cross_entropy_loss(scores: jnp.ndarray, target: jnp.ndarray):\n",
    "    scores -= scores.max()\n",
    "    scores = jnp.exp(scores)\n",
    "    probs = scores / scores.sum()\n",
    "    return -jnp.log(probs[target])\n",
    "\n",
    "def forward(params: list[Tuple], input: jnp.ndarray, target: jnp.ndarray):\n",
    "    for W, b in params:\n",
    "        output = input.dot(W) + b\n",
    "        input = jnp.maximum(0, output)\n",
    "    return cross_entropy_loss(output, target)\n",
    "\n",
    "def batched_forward(params, X, y):\n",
    "    return jnp.mean(vmap(forward, in_axes=[None, 0, 0])(params, X, y))\n",
    "\n",
    "def init_layer(Din, Dout, key):\n",
    "    return (jnp.sqrt(2/(Din*Dout)) * random.normal(key, (Din, Dout)), jnp.zeros(Dout))\n",
    "\n",
    "@jit\n",
    "def update(params, config) -> list[Tuple]:\n",
    "    loss, grads = value_and_grad(batched_forward)(params, X, y)\n",
    "    new_params = []\n",
    "    new_config = []\n",
    "    for l in range(2):\n",
    "        W, W_config = adam(params[l][0], grads[l][0], config[l][0])\n",
    "        b, b_config = adam(params[l][1], grads[l][1], config[l][1])\n",
    "        new_params.append((W,b))\n",
    "        new_config.append((W_config, b_config))\n",
    "    return new_params, new_config, loss\n",
    "\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "key, *subkeys = random.split(key, 3)\n",
    "params = [init_layer(D, 100, subkeys[0]), init_layer(100, K, subkeys[1])]\n",
    "\n",
    "\"\"\"\n",
    "W1 = make_default_adam_config(params[0][0])\n",
    "b1 = make_default_adam_config(params[0][1])\n",
    "W2 = make_default_adam_config(params[1][0])\n",
    "b2 = make_default_adam_config(params[1][1])\n",
    "config = [(W1,b1),(W2,b2)]\n",
    "\"\"\"\n",
    "config = [({},{}),({},{})]\n",
    "for t in range(100):\n",
    "    params, config, loss = update(params, config)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "from PredCodMLP import cross_entropy_loss\n",
    "\n",
    "num_train, C, H, W = train_images.shape\n",
    "K = 10\n",
    "batch_size = 256\n",
    "test_batch = np.array(test_images, dtype=np.float32).reshape(num_test, -1)\n",
    "test_batch /= (255/2)\n",
    "test_batch -= 1\n",
    "\n",
    "predcod = PredCodMLP([H*W, 800, K])\n",
    "for t in range(1000):\n",
    "    batch_mask = np.random.choice(num_train, batch_size)\n",
    "    batch = np.array(train_images[batch_mask], dtype=np.float32)\n",
    "    batch = batch.reshape(batch_size, -1)\n",
    "    batch /= (255/2)\n",
    "    batch -= 1\n",
    "    labels = train_labels[batch_mask]\n",
    "    predcod.train_step(batch, labels, 1e-3)\n",
    "    if t % 100 == 0:\n",
    "        loss, _ = cross_entropy_loss(predcod.predict(test_batch), test_labels)\n",
    "        print(loss)\n",
    "#classes = np.argmax(preds, 1)\n",
    "#print(np.mean(y.flatten() == classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd fast_layers\n",
    "%conda activate predcod\n",
    "!python setup.py build_ext --inplace\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:  0 training accuracy:  0.159423828125\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "from cnn import CNN\n",
    "from adam import adam\n",
    "\n",
    "num_train, C, H, W = train_images.shape\n",
    "batch_size = 512\n",
    "num_filters = 25\n",
    "num_classes = 10\n",
    "cnn = CNN(batch_size, C, H, W, num_filters, num_classes)\n",
    "config = { 'W1': {}, 'b1': {}, 'W2': {}, 'b2': {} }\n",
    "for t in range(1000):\n",
    "    batch_mask = np.random.choice(num_train, batch_size)\n",
    "    batch = np.array(train_images[batch_mask], dtype=np.float32)\n",
    "    batch /= (255/2)\n",
    "    batch -= 1\n",
    "    labels = train_labels[batch_mask]\n",
    "    loss, dscores, cache = cnn.loss(batch, labels)\n",
    "    grads = cnn.backward(dscores, cache)\n",
    "    lr = 1e-3\n",
    "    cnn.W1, config['W1'] = adam(cnn.W1, grads['W1'], config['W1'])\n",
    "    cnn.b1, config['b1'] = adam(cnn.b1, grads['b1'], config['b1'])\n",
    "    cnn.W2, config['W2'] = adam(cnn.W2, grads['W2'], config['W2'])\n",
    "    cnn.b2, config['b2'] = adam(cnn.b2, grads['b2'], config['b2'])\n",
    "    if t % 100 == 0:\n",
    "        batch_mask = np.random.choice(num_train, 2**12)\n",
    "        batch = np.array(train_images[batch_mask], dtype=np.float32)\n",
    "        batch_labels = train_labels[batch_mask]\n",
    "        #batch = np.array(train_images, dtype=np.float32)\n",
    "        batch /= (255/2)\n",
    "        batch -= 1\n",
    "        scores, _ = cnn.forward(batch)\n",
    "        train_acc = np.mean(np.argmax(scores, 1) == batch_labels)\n",
    "        print(\"t: \", t, \"training accuracy: \", train_acc)\n",
    "        scores, _ = cnn.forward(test_images)\n",
    "        test_acc = np.mean(np.argmax(scores, 1) == test_labels)\n",
    "        print(\"validation accuracy: \", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "\"\"\"\n",
    "scores, _ = cnn.forward(train_images)\n",
    "train_acc = np.mean(np.argmax(scores, 1) == train_labels)\n",
    "print(\"training accuracy: \", train_acc)\n",
    "\"\"\"\n",
    "scores, _ = cnn.forward(test_images)\n",
    "test_acc = np.mean(np.argmax(scores, 1) == test_labels)\n",
    "print(\"validation accuracy: \", test_acc)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b47fb9f8a43b2379e79497425d2a2e979e2426e74b41ed9428e9101717abc52"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('predcod': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}